{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shapi88/tensorflow_book/blob/main/ml_resources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE_7aYzB6q79"
      },
      "source": [
        "# ðŸ“– Resources\n",
        "\n",
        "### Documentation & APIs\n",
        "* ðŸ“– [Matrix Multiplication](https://www.mathsisfun.com/algebra/matrix-multiplying.html)\n",
        "* ðŸ“– [NumPy](https://numpy.org/doc/stable/reference/index.html)\n",
        "* ðŸ“– [Tensorflow](https://www.tensorflow.org/api_docs/python/tf)\n",
        "* ðŸ“– [Keras](https://www.keras.io)\n",
        "* ðŸ“– [Keras Transfer Learning](https://keras.io/guides/transfer_learning/#build-a-model)\n",
        "* ðŸ“– [Tensorflow Decision Forest](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/all_symbols)\n",
        "\n",
        "* ðŸ“– [Pandas](https://pandas.pydata.org/docs/reference/index.html#api)\n",
        "* ðŸ“– [Seaborn](https://seaborn.pydata.org/)\n",
        "* ðŸ“– [Matplotlib](https://matplotlib.org/stable/plot_types/index.html)\n",
        "* ðŸ“– [Scipy](https://docs.scipy.org/doc/scipy/reference/index.html#scipy-api)\n",
        "* ðŸ“– [Scikit Learn](https://scikit-learn.org/stable/)\n",
        "* ðŸ“– [Missingno](https://github.com/ResidentMario/missingno)\n",
        "* ðŸ“– [Daniel Bourke](https://www.mrdbourke.com)\n",
        "* ðŸ“– [Cheat Sheet Activation Function](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
        "* ðŸ“– [Papers w/ code SOTA](https://paperswithcode.com/sota)\n",
        "* ðŸ“– [Huggingface](https://huggingface.co/)\n",
        "* ðŸ“– [resnet v2](https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5)\n",
        "* ðŸ“– [efficientnet](https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1)\n",
        "* ðŸ“– [EfficientNet Google Blog AI](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)\n",
        "\n",
        "### Videos\n",
        "* ðŸŽ¥ [10 crazy announcements from Google I/O - Fireship](https://www.youtube.com/watch?v=nmfRDRNjCnM)\n",
        "* ðŸŽ¥ [Daniel Bourke YT Channel](https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ)\n",
        "* ðŸŽ¥ [MIT 6.S191 (2022): Convolutional Neural Networks](https://www.youtube.com/watch?v=uapdILWYTzE&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=4)\n",
        "* ðŸŽ¥ [mini-batch gradient descent](https://www.youtube.com/watch?v=-_4Zi8fCZO4)\n",
        "\n",
        "### Learning\n",
        "* ðŸ“– [kaggle](https://www.kaggle.com/)\n",
        "* ðŸ“– [ztm](https://zerotomastery.io/)\n",
        "* ðŸ“– [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)\n",
        "\n",
        "### Tools\n",
        "* ðŸ›  [Matrix Multiplication calculator](http://matrixmultiplication.xyz/)\n",
        "* ðŸ›  [Google Collab](https://colab.research.google.com/)\n",
        "* ðŸ›  [ChatGPT](https://chat.openai.com/)\n",
        "* ðŸ›  [TF Playground](https://playground.tensorflow.org/)\n",
        "* ðŸ›  [NN Case Study](https://cs231n.github.io/neural-networks-case-study/)\n",
        "* ðŸ›  [Multi Layer Perceptron](https://github.com/GokuMohandas/Made-With-ML/blob/main/notebooks/08_Neural_Networks.ipynb)\n",
        "* ðŸ›  [TensoBoard](https://tensorboard.dev)\n",
        "* ðŸ›  [Weights & Biases](https://wandb.ai/site)\n",
        "\n",
        "### Dojo\n",
        "\n",
        "* ðŸ¥‹ [mrdbourke/tensorflow-deep-learning Exercises](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/README.md#-02-neural-network-classification-with-tensorflow-exercises)\n",
        "* ðŸ¥‹ [mrdbourke/tensorflow-deep-learning Discussion Page](https://github.com/mrdbourke/tensorflow-deep-learning/discussions)\n",
        "* ðŸ¥‹ [Data Augmentation Tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation)\n",
        "* ðŸ¥‹ [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n",
        "\n",
        "\n",
        "### Twitters\n",
        "\n",
        "* ðŸªº [@ylecun, Researcher in AI, Machine Learning, Robotics](https://twitter.com/ylecun)\n",
        "\n",
        "### How to\n",
        "* [Hot load images using tensorflow](https://www.tensorflow.org/tutorials/load_data/images)\n",
        "\n",
        "### Papers\n",
        "* ðŸ“– [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\n",
        "\n",
        "* ðŸ“– [ResNet - Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027v3.pdf)\n",
        "* ðŸ“– [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸ¥µ TO-DO\n",
        "\n",
        "Check those:\n",
        "* ðŸ›  NN Case Study\n",
        "* ðŸ›  Multi Layer Perceptron\n",
        "* ðŸ›  Activation Functions Formulas"
      ],
      "metadata": {
        "id": "76cpuSIxyCMU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr4kfHyDLkFX"
      },
      "source": [
        "# ðŸ’¡ Hyperparameters\n",
        "* Hyperparameters are parameters that are set before training a model and determine how the model is trained. These are not learned during training, but are specified by the user before training begins.\n",
        "\n",
        "* Hyperparameters are used to control aspects of the model's behavior, such as the learning rate, regularization strength, number of hidden layers, and number of nodes in each layer of a neural network. The choice of hyperparameters can have a significant impact on the model's performance, and finding the optimal values for them can be a challenging task that requires experimentation and tuning.\n",
        "\n",
        "* Hyperparameter tuning is the process of finding the best values for these parameters by adjusting them and training the model repeatedly, evaluating its performance each time. The goal is to find the hyperparameter values that result in the best performing model on a given task, such as classification or regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnGhZDmz1Dv8"
      },
      "source": [
        "# ðŸ”‘ Models\n",
        "There are many different types of machine learning models that exist, each with its own strengths and weaknesses. Here are some common types of machine learning models:\n",
        "\n",
        "* **Linear Regression**: A type of model used for predicting a continuous output value based on one or more input features.\n",
        "\n",
        "* **Logistic Regression**: A type of model used for binary classification problems, where the output variable takes one of two possible values.\n",
        "\n",
        "* **Decision Trees**: A type of model that makes decisions by recursively splitting the data based on the values of the input features.\n",
        "\n",
        "* **Random Forests**: An ensemble model that combines multiple decision trees to make predictions.\n",
        "\n",
        "* **Support Vector Machines (SVM)**: A model that finds the best hyperplane to separate the data into different classes.\n",
        "\n",
        "* **Naive Bayes**: A model that calculates the probability of each class based on the input features using Bayes' theorem.\n",
        "\n",
        "* **Neural Networks**: A type of model that consists of interconnected nodes that process information and can learn complex patterns in the data.\n",
        "\n",
        "* **Clustering Models**: A type of model used to group similar data points together based on their similarity.\n",
        "\n",
        "* **Dimensionality Reduction Models**: A type of model used to reduce the number of input features to a more manageable number.\n",
        "\n",
        "* **Reinforcement Learning Models**: A type of model used to learn from experience through a process of trial and error to maximize a reward signal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvU5wrZL2JKi"
      },
      "source": [
        "## Linear Regression Model\n",
        "\n",
        "This model uses TensorFlow's Keras API to create a sequential model with a single dense layer. The `units` argument specifies the number of neurons in the layer, and the `input_shape` argument specifies the shape of the input data.\n",
        "\n",
        "The model is compiled with stochastic gradient descent (SGD) as the optimizer and mean squared error (MSE) as the loss function. The model is then trained on the training data (`x_train` and `y_train`) for 100 epochs.\n",
        "\n",
        "Finally, the model is used to make a prediction on new data (`x_test`), and the predicted output is printed to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t9yeyhK1DKP",
        "outputId": "02dc00a3-f891-48b3-a371-7dec4a4a03c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 62ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[13.381614]], dtype=float32)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the input and output variables\n",
        "x_train = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
        "y_train = [3.0, 5.0, 7.0, 9.0, 11.0]\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.SGD(learning_rate=0.01), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=100, verbose=0)\n",
        "\n",
        "# Make a prediction\n",
        "x_test = [6.0]\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6FvXYDl4rqL"
      },
      "source": [
        "## Logistic Regression Model\n",
        "\n",
        "We have a dataset with two features and binary classification labels. We define a logistic regression model using the `tf.keras.Sequential()` API, with a single `Dense` layer with one output unit and a sigmoid activation function.\n",
        "\n",
        "The model is compiled using stochastic gradient descent (`SGD`) as the optimizer and binary cross-entropy as the loss function. We then train the model on the training data using the `fit()` method.\n",
        "\n",
        "Finally, we use the trained model to make predictions on new data (`X_test`), and the predicted output is printed to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnnGf2qg421O",
        "outputId": "5bdeb782-dff3-4f01-c5f3-d311c59d8f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 45ms/step\n",
            "[[0.9735141]\n",
            " [0.9887447]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "y_train = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(1, input_dim=2, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "\n",
        "# Use the model to make predictions\n",
        "X_test = np.array([[6, 7], [7, 8]])\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOMES7bQ6E2m"
      },
      "source": [
        "## Decision Trees Model\n",
        "In this example, we're using the Iris dataset and creating a decision tree model using a neural network. We define the model using the `Sequential` API and add two fully connected layers with `Dense`. The first layer has 16 units and `ReLU` activation, while the second layer has 3 units and `softmax` activation, which is suitable for multiclass classification.\n",
        "\n",
        "The model is then compiled with the `adam` optimizer and `sparse_categorical_crossentropy` as the loss function. We then train the model on the entire dataset for 100 epochs.\n",
        "\n",
        "Finally, we evaluate the model on the same dataset and print out the test loss and accuracy. Note that in practice, it is usually better to split the data into separate training and testing sets to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hgd7hfU6Ia1",
        "outputId": "238a4407-3d0c-4632-aaa3-a3b29a0a7230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 0.4234 - accuracy: 0.9200\n",
            "Test loss: 0.42342260479927063, Test accuracy: 0.9200000166893005\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Test loss: {loss}, Test accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYMxrmsb6jAG"
      },
      "source": [
        "## Random Forests Model\n",
        "\n",
        "In this example, we're using the Iris dataset and creating a Random Forest model using scikit-learn's `RandomForestClassifier`. We set the number of trees to be 100 by setting `n_estimators=100`.\n",
        "\n",
        "We then train the model on the entire dataset using the `fit` method.\n",
        "\n",
        "Finally, we evaluate the model on the same dataset using the `score` method, which returns the mean accuracy on the given data and labels. Note that in practice, it is usually better to split the data into separate training and testing sets to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7b33rjU6juK",
        "outputId": "bc1f17c9-8faa-401a-fa0e-b292ccf7e8ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create a Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.score(X, y)\n",
        "print(f\"Test accuracy: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgH6bwnb7Dlq"
      },
      "source": [
        "## Support Vector Machines (SVM) Model\n",
        "\n",
        "In this example, we're using the Iris dataset and creating a SVM model using scikit-learn's `SVC`. We set the kernel to be linear by setting `kernel='linear'` and the regularization parameter `C` to be 1 by setting `C=1`.\n",
        "\n",
        "We then train the model on the entire dataset using the `fit` method.\n",
        "\n",
        "Finally, we evaluate the model on the same dataset using the `score` method, which returns the mean accuracy on the given data and labels. Note that in practice, it is usually better to split the data into separate training and testing sets to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IBKCF207qO3",
        "outputId": "ce74532a-3243-49b9-d147-44b417ae015d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9933333333333333\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create a SVM model\n",
        "model = SVC(kernel='linear', C=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.score(X, y)\n",
        "print(f\"Test accuracy: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJtOUDvV7t_e"
      },
      "source": [
        "## Neural Networks Model\n",
        "\n",
        "In this example, we're using the Iris dataset and creating a Neural Network model using TensorFlow's Keras API. We're using a sequential model with two dense layers - the first with 64 units and a ReLU activation function, and the second with 3 units and a softmax activation function to output the predicted class probabilities.\n",
        "\n",
        "We compile the model with the 'adam' optimizer, the 'sparse_categorical_crossentropy' loss function (since our labels are integers), and the 'accuracy' metric to monitor during training.\n",
        "\n",
        "We then train the model on the training data for 100 epochs, using the validation set to monitor the model's performance during training.\n",
        "\n",
        "Finally, we evaluate the model on the test set using the `evaluate` method, which returns the loss and accuracy of the model on the given test data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "xfv5_bv_U91h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers Hyperparameters\n",
        "\n",
        "* **Padding** is often necessary when the kernel extends beyond the activation map. Padding conserves data at the borders of activation maps, which leads to better performance, and it can help preserve the input's spatial size, which allows an architecture designer to build depper, higher performing networks. There exist many padding techniques, but the most commonly used approach is zero-padding because of its performance, simplicity, and computational efficiency. The technique involves adding zeros symmetrically around the edges of an input. This approach is adopted by many high-performing CNNs such as AlexNet.\n",
        "\n",
        "* **Kernel** size, often also referred to as filter size, refers to the dimensions of the sliding window over the input. Choosing this hyperparameter has a massive impact on the image classification task. For example, small kernel sizes are able to extract a much larger amount of information containing highly local features from the input. As you can see on the visualization above, a smaller kernel size also leads to a smaller reduction in layer dimensions, which allows for a deeper architecture. Conversely, a large kernel size extracts less information, which leads to a faster reduction in layer dimensions, often leading to worse performance. Large kernels are better suited to extract features that are larger. At the end of the day, choosing an appropriate kernel size will be dependent on your task and dataset, but generally, smaller kernel sizes lead to better performance for the image classification task because an architecture designer is able to stack more and more layers together to learn more and more complex features!\n",
        "\n",
        "* **Stride** indicates how many pixels the kernel should be shifted over at a time. For example, as described in the convolutional layer example above, Tiny VGG uses a stride of 1 for its convolutional layers, which means that the dot product is performed on a 3x3 window of the input to yield an output value, then is shifted to the right by one pixel for every subsequent operation. The impact stride has on a CNN is similar to kernel size. As stride is decreased, more features are learned because more data is extracted, which also leads to larger output layers. On the contrary, as stride is increased, this leads to more limited feature extraction and smaller output layer dimensions. One responsibility of the architecture designer is to ensure that the kernel slides across the input symmetrically when implementing a CNN. Use the hyperparameter visualization above to alter stride on various input/kernel dimensions to understand this constraint!\n",
        "\n",
        "### ELI5:\n",
        "> * **Padding**: Imagine you're coloring a picture, but some parts are missing around the edges. Padding is like adding extra space around the picture so you can color everything nicely without going outside the lines.\n",
        "\n",
        "> * **Kernel Size:**\n",
        "When you want to look closely at something, you might use a small magnifying glass. In a computer, a kernel is like a special magnifying glass that helps us see small details in pictures. The kernel size is like the size of that magnifying glassâ€”smaller sizes let us see tiny things, and larger sizes show bigger things.\n",
        "\n",
        "> * **Stride:**\n",
        "When you take steps, you move from one place to another. Stride is like the size of your steps when you walk. In a computer, stride is about how far we move the special magnifying glass (the kernel) when we look at a picture. Small strides mean we take small steps and see lots of details, while big strides mean we take bigger steps and see less detail."
      ],
      "metadata": {
        "id": "dL4GaLCDVDFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet Implementation"
      ],
      "metadata": {
        "id": "f-SQ8GsPIcji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a ResNet, we start by putting some blocks together to form the base of our tower. This is like the initial part of the network that helps us understand the basic features of the input, like the shape and color.\n",
        "\n",
        "\n",
        "Then, we start adding more blocks on top of the base. But here's the interesting part: instead of just adding one block on top of another, we create something called a \"residual block.\"\n",
        "\n",
        "\n",
        "A residual block is like a special kind of block that keeps a connection to the previous block. It's like having a shortcut between the blocks. This shortcut helps us in two ways:\n",
        "\n",
        "* It allows us to learn more complex features. As we go higher in the tower, we can start combining the basic features we learned in the base to form more intricate patterns. It's like using the building blocks in different ways to create more interesting shapes and structures.\n",
        "\n",
        "* It helps us avoid getting lost or confused. Sometimes, as we add more blocks, we can make mistakes or lose important information. But with the shortcut connection, we can go back to the previous block and make corrections or use the information that we might have missed. It's like having a guide or map that shows us the right path if we get stuck.\n",
        "\n",
        "By using these residual blocks with shortcut connections, we can build a deeper and more powerful tower (or neural network). This helps us understand and recognize more complex patterns in the input data, like identifying different objects in images or understanding the meaning of words in text.\n",
        "\n",
        "So, in simple terms, a ResNet is like building a tower of blocks where each block remembers what was learned before and helps us build more complex structures. This allows us to solve more challenging problems and make better predictions.\n",
        "\n",
        "* ðŸ“– [ResNet - Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027v3.pdf)\n",
        "\n",
        "> ðŸ”‘ **Note**: Basic implementation of a Deep Residual Network (ResNet) in TensorFlow. Here's an example of a ResNet with multiple residual blocks:"
      ],
      "metadata": {
        "id": "8QifvfdhIrE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def residual_block(x, filters, strides=1, use_projection=False):\n",
        "    identity = x\n",
        "\n",
        "    # Projection shortcut to match dimensions (if needed)\n",
        "    if use_projection:\n",
        "        identity = layers.Conv2D(filters, kernel_size=1, strides=strides, padding='same')(identity)\n",
        "        identity = layers.BatchNormalization()(identity)\n",
        "\n",
        "    # First convolutional layer\n",
        "    x = layers.Conv2D(filters, kernel_size=3, strides=strides, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Second convolutional layer\n",
        "    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Adding the identity shortcut to the output\n",
        "    x = layers.add([x, identity])\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_resnet(input_shape, num_classes):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolutional layer\n",
        "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = residual_block(x, filters=64, strides=1, use_projection=False)\n",
        "    x = residual_block(x, filters=64, strides=1, use_projection=False)\n",
        "    x = residual_block(x, filters=64, strides=1, use_projection=False)\n",
        "\n",
        "    x = residual_block(x, filters=128, strides=2, use_projection=True)\n",
        "    x = residual_block(x, filters=128, strides=1, use_projection=False)\n",
        "    x = residual_block(x, filters=128, strides=1, use_projection=False)\n",
        "\n",
        "    x = residual_block(x, filters=256, strides=2, use_projection=True)\n",
        "    x = residual_block(x, filters=256, strides=1, use_projection=False)\n",
        "    x = residual_block(x, filters=256, strides=1, use_projection=False)\n",
        "\n",
        "    x = residual_block(x, filters=512, strides=2, use_projection=True)\n",
        "    x = residual_block(x, filters=512, strides=1, use_projection=False)\n",
        "    x = residual_block(x, filters=512, strides=1, use_projection=False)\n",
        "\n",
        "    # Final layers\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SHtzRqbVIhm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The build_resnet function can be used to create a ResNet model by specifying the input shape and the number of classes for your classification task."
      ],
      "metadata": {
        "id": "ta1nb78HIp_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (224, 224, 3)  # Example input shape for RGB images\n",
        "num_classes = 10  # Example number of classes\n",
        "\n",
        "model = build_resnet(input_shape, num_classes)\n"
      ],
      "metadata": {
        "id": "wUvKul4ZIpNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There are two ways to instantiate a Model:\n",
        "\n",
        "more info at [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) or [Keras Build a Model](https://keras.io/guides/transfer_learning/#build-a-model)"
      ],
      "metadata": {
        "id": "a8oo90EPJAFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = keras.applications.Xception(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False,\n",
        ")  # Do not include the ImageNet classifier at the top.\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "# Pre-trained Xception weights requires that input be scaled\n",
        "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
        "# outputs: `(inputs * scale) + offset`\n",
        "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "x = scale_layer(x)\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here.\n",
        "x = base_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "47n9hsHcDHCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 - With the \"Functional API\"\n",
        "\n",
        "Where you start from Input, you chain layer calls to specify the model's forward pass, and finally you create your model from inputs and outputs:"
      ],
      "metadata": {
        "id": "SCbGFFUq60k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "inputs = tf.keras.Input(shape=(3,))\n",
        "x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
        "outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "6iDXvIFa6NHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\n",
        "\n",
        "**Example:**"
      ],
      "metadata": {
        "id": "dT-KegLc61DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None, None, 3))\n",
        "processed = keras.layers.RandomCrop(width=32, height=32)(inputs)\n",
        "conv = keras.layers.Conv2D(filters=2, kernel_size=3)(processed)\n",
        "pooling = keras.layers.GlobalAveragePooling2D()(conv)\n",
        "feature = keras.layers.Dense(10)(pooling)\n",
        "\n",
        "full_model = keras.Model(inputs, feature)\n",
        "backbone = keras.Model(processed, conv)\n",
        "activations = keras.Model(conv, feature)"
      ],
      "metadata": {
        "id": "sbIcvsBr7CTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ”‘ **Note**: that the backbone and activations models are not created with keras.Input objects, but with the tensors that are originated from keras.Input objects. Under the hood, the layers and weights will be shared across these models, so that user can train the full_model, and use backbone or activations to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs."
      ],
      "metadata": {
        "id": "QuJn1ltw7F8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 - By subclassing the Model class\n",
        "\n",
        "In that case, you should define your layers in __init__() and you should implement the model's forward pass in call()."
      ],
      "metadata": {
        "id": "Jkg1krAO7Sxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    return self.dense2(x)\n",
        "\n",
        "model = MyModel()\n"
      ],
      "metadata": {
        "id": "2d1Oo7em7Rx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you subclass Model, you can optionally have a training argument (boolean) in call(), which you can use to specify a different behavior in training and inference:"
      ],
      "metadata": {
        "id": "sUVp8YOn7aup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    x = self.dense1(inputs)\n",
        "    if training:\n",
        "      x = self.dropout(x, training=training)\n",
        "    return self.dense2(x)\n",
        "\n",
        "model = MyModel()"
      ],
      "metadata": {
        "id": "HWqdhVHt7bKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is created, you can config the model with losses and metrics with model.compile(), train the model with model.fit(), or use the model to do prediction with model.predict()."
      ],
      "metadata": {
        "id": "xLYnHr7e7jkP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHBzT6hQv6lSyPoWaH33JG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}